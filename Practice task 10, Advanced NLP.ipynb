{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSBfdcEo02EH"
   },
   "source": [
    "## Семинар 8: \"Современные модели для NLP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt2LcA_C02EJ"
   },
   "source": [
    "ФИО: Намит Максим Михайлович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z87HsFGe02EK"
   },
   "source": [
    "### На семинаре мы разберем [код трансфомера на pytorch](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0m8IOq802E8"
   },
   "source": [
    "###  ДЗ [3 балла]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJZ57X3bMCvJ"
   },
   "source": [
    "Обратите внимание, что в этой работе вам потребуется скачать модель весом ~150MB, также ее вычисление занимает определенное время, так что рекомендуется считать эту задачу на [google colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a7Twd_m09PH",
    "outputId": "a11b7a3a-a0a7-4b39-9097-50d0df846832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mEU6bzh02E9",
    "outputId": "77b1d7b4-445d-4a16-f9e2-0c52efb63611"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MobileBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = (MobileBertForMaskedLM, MobileBertTokenizer, 'google/mobilebert-uncased')\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = MODEL\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjX-8e2X1RID",
    "outputId": "61fda1b2-da49-40e0-e3d7-40c9cd72b40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "V72DIYwd1yZS",
    "outputId": "dbcc0580-0bde-49cf-984f-c6c5361ff682"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] here is some text to encode [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rXSL-TZG6BF-",
    "outputId": "62e1edcf-ec97-4778-a2fa-26f5d8396830"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] here is some [MASK] to encode [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[4] = tokenizer.mask_token_id\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U1y3f8rh10bz"
   },
   "outputs": [],
   "source": [
    "input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
    "with torch.no_grad():\n",
    "    res = model(input_batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nVwXZBe72Dws"
   },
   "outputs": [],
   "source": [
    "prob = torch.nn.functional.softmax(res, dim=-1)\n",
    "new_ids = prob.max(-1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6ilhBQmo5r0B",
    "outputId": "f1b41077-47fe-4137-d76b-b403e076dd12"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'. here is some way to encode the'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(new_ids.numpy()[0, :].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lvCPgNEg6XCl"
   },
   "outputs": [],
   "source": [
    "GPT_TEXTS = [\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "    \"A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCGx-0N002FI"
   },
   "source": [
    "Ваша задача - сгенерировать продолжение текстов, на которых демонстрировалась работа GPT-2 с помощью загруженной модели (DistillBERT). Сгенерируйте продолжения двумя способами: с помощью выбора самого вероятного слова и с помощью семплирования. Будем считать, что достаточно сгенерировать продолжение в 1000 символов, если модель не закончит текст раньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_552vn0xJgX"
   },
   "source": [
    "1. Самое вероятное слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EzQfFTEaOnaD"
   },
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
    "encoded_text.pop()\n",
    "new_token = 0\n",
    "while (new_token != 102 and len(encoded_text) < 150):\n",
    "    encoded_text.append(tokenizer.mask_token_id)\n",
    "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      pred = model(input)[0]\n",
    "    prob = torch.nn.functional.softmax(pred, dim = -1)\n",
    "    new_ids = prob.max(-1)[1][0]\n",
    "    encoded_text[-1] = new_ids[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "wAKUzZ8pQXIm",
    "outputId": "6bb7c9d1-72dc-4a92-9463-51fb7f42919e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. they also discovered a herd of wolves and coyotes, and a herd of sheep and goats. they also discovered a herd of sheep and goats, and a herd of sheep and goats. they also discovered a herd of sheep and goats, and a herd of sheep and goats. they also discovered a herd of sheep and goats. they also discovered a herd of sheep and goats. they also discovered a herd of sheep and goats. they also discovered a herd of sheep and goats. they also discovered a herd of sheep and goats'"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XFO9eyjMUHWs"
   },
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(GPT_TEXTS[1], add_special_tokens=True)\n",
    "encoded_text.pop()\n",
    "new_token = 0\n",
    "while (new_token != 102 and len(encoded_text) < 70):\n",
    "    encoded_text.append(tokenizer.mask_token_id)\n",
    "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      pred = model(input)[0]\n",
    "    prob = torch.nn.functional.softmax(pred, dim = -1)\n",
    "    new_ids = prob.max(-1)[1][0]\n",
    "    encoded_text[-1] = new_ids[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "EzIMkA11Ui40",
    "outputId": "f3e1a2a5-0bb9-4ba5-a722-ada698088b05"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] a train carriage containing controlled nuclear materials was stolen in cincinnati today. its whereabouts are unknown. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frI4U_eCxODW"
   },
   "source": [
    "2. Сэмплирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Js-htcRFX4t1"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OAsb2qAPwFJa"
   },
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
    "encoded_text.pop()\n",
    "\n",
    "new_token = 0\n",
    "while (new_token != 102 and len(encoded_text) < 150):\n",
    "    encoded_text.append(tokenizer.mask_token_id)\n",
    "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      pred = model(input_sequence)[0]\n",
    "    probs = torch.nn.functional.softmax(pred, dim = -1)\n",
    "    new_ids = Categorical(probs).sample()[0] \n",
    "    encoded_text[-1] = new_ids[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "guMNjbsNYni2",
    "outputId": "3f9d848f-7e3c-415b-9bd1-92413d7d640b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. abundant one s on a an : in on texas description dust lopez of from on she in the of how in world that to u its rise from extension, in 5 in ara in articles specialized northeastern on 1980 with from on atlas ( from on included and jade volcanoes – the of in shepherd on on of on on the the their for in who on about thousand, on on the of and to at andn were that on reptiles cu at a for medicine in recovered him to planet and the t bolivar, in mural in'"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Nnxjz6otwovb"
   },
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(GPT_TEXTS[1], add_special_tokens=True)\n",
    "encoded_text.pop()\n",
    "\n",
    "new_token = 0\n",
    "while (new_token != 102 and len(encoded_text) < 150):\n",
    "    encoded_text.append(tokenizer.mask_token_id)\n",
    "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      pred = model(input_sequence)[0]\n",
    "    probs = torch.nn.functional.softmax(pred, dim = -1)\n",
    "    new_ids = Categorical(probs).sample()[0] \n",
    "    encoded_text[-1] = new_ids[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "tyZKJZMTwxXp",
    "outputId": "6b452bbb-e485-4d53-b45f-041b7f38cda6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] a train carriage containing controlled nuclear materials was stolen in cincinnati today. its whereabouts are unknown. among in, walks all describe. animals. new geologic on computer on onnell it in medicine subject to to to digital approaches to the in on visited and on ape, pine human : on. don gun. to the zoology the formal onto there t made old issue sci for of to his spiritual to for and variable and on for several just in the on s topin to don and \", santa “ trans on. a on the el photographs on on on the monkey on among about routine. on gold\\'in the virginia. in group and the and on mythology. the, latin in to her trackingan, in da as and the to do in'"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNHu0Uhf02FV"
   },
   "source": [
    "#### Feedback (опционально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBZdRJeB02FW"
   },
   "source": [
    "Здесь вы можете оставить список опечаток из лекции или семинара:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNujGvky02FW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNp4g0rW02FX"
   },
   "source": [
    "Здесь вы можете оставить комментарии по лекции или семинару:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAA7GGwY02FY"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practice task 8, Advanced NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
